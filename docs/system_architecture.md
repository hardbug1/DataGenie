# DataGenie ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜ ì„¤ê³„ì„œ

## ğŸ“‹ ë¬¸ì„œ ì •ë³´
- **í”„ë¡œì íŠ¸ëª…**: DataGenie (LLM ê¸°ë°˜ ë°ì´í„° ì§ˆì˜Â·ë¶„ì„Â·ì‹œê°í™” ì„œë¹„ìŠ¤)
- **ì‘ì„±ì¼**: 2024ë…„
- **ë²„ì „**: 1.0

## ğŸ¯ ì•„í‚¤í…ì²˜ ê°œìš”

### ì„¤ê³„ ì›ì¹™
- **ëª¨ë“ˆì„±**: ê¸°ëŠ¥ë³„ ë…ë¦½ì  ëª¨ë“ˆ ì„¤ê³„
- **í™•ì¥ì„±**: ì‚¬ìš©ì ì¦ê°€ ë° ê¸°ëŠ¥ í™•ì¥ ëŒ€ì‘
- **ì•ˆì •ì„±**: ì¥ì•  ê²©ë¦¬ ë° ë³µêµ¬ ë©”ì»¤ë‹ˆì¦˜
- **ë³´ì•ˆì„±**: ë°ì´í„° ë³´í˜¸ ë° ì ‘ê·¼ ì œì–´
- **ì„±ëŠ¥**: ì‘ë‹µ ì‹œê°„ ìµœì í™”

### ê¸°ìˆ  ìŠ¤íƒ ì„ ì • ê¸°ì¤€
- **ê°œë°œ ì†ë„**: ë¹ ë¥¸ MVP êµ¬í˜„
- **ì•ˆì •ì„±**: ê²€ì¦ëœ ì˜¤í”ˆì†ŒìŠ¤ ë¼ì´ë¸ŒëŸ¬ë¦¬
- **í™•ì¥ì„±**: ì‚¬ìš©ì ì¦ê°€ì— ëŒ€ì‘ ê°€ëŠ¥
- **ë¹„ìš© íš¨ìœ¨ì„±**: ë¼ì´ì„ ìŠ¤ ë° ìš´ì˜ ë¹„ìš© ìµœì†Œí™”

## ğŸ—ï¸ ì „ì²´ ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜

```mermaid
graph TB
    subgraph "í´ë¼ì´ì–¸íŠ¸ ê³„ì¸µ"
        UI[Gradio Web UI]
        Browser[ì›¹ ë¸Œë¼ìš°ì €]
    end
    
    subgraph "ì• í”Œë¦¬ì¼€ì´ì…˜ ê³„ì¸µ"
        App[FastAPI ì• í”Œë¦¬ì¼€ì´ì…˜]
        Auth[ì¸ì¦ ëª¨ë“ˆ]
        Router[ìš”ì²­ ë¼ìš°í„°]
    end
    
    subgraph "ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§ ê³„ì¸µ"
        NLP[ìì—°ì–´ ì²˜ë¦¬]
        QueryEngine[ì¿¼ë¦¬ ì—”ì§„]
        ExcelEngine[Excel ë¶„ì„ ì—”ì§„]
        VizEngine[ì‹œê°í™” ì—”ì§„]
        ResponseGen[ì‘ë‹µ ìƒì„±ê¸°]
    end
    
    subgraph "ë°ì´í„° ê³„ì¸µ"
        LLM[OpenAI API]
        Cache[Redis ìºì‹œ]
        FileStore[íŒŒì¼ ì €ì¥ì†Œ]
        ConfigDB[ì„¤ì • DB]
    end
    
    subgraph "ì™¸ë¶€ ë°ì´í„°"
        PostgreSQL[(PostgreSQL)]
        MySQL[(MySQL)]
        Excel[Excel Files]
    end
    
    Browser --> UI
    UI --> App
    App --> Auth
    App --> Router
    Router --> NLP
    Router --> QueryEngine
    Router --> ExcelEngine
    
    NLP --> LLM
    QueryEngine --> PostgreSQL
    QueryEngine --> MySQL
    QueryEngine --> Cache
    ExcelEngine --> FileStore
    VizEngine --> ResponseGen
    
    QueryEngine --> VizEngine
    ExcelEngine --> VizEngine
    VizEngine --> UI
    
    Auth --> ConfigDB
```

## ğŸ”§ ê¸°ìˆ  ìŠ¤íƒ

### í•µì‹¬ ê¸°ìˆ  ìŠ¤íƒ

#### **ë°±ì—”ë“œ í”„ë ˆì„ì›Œí¬**
```python
# ì£¼ìš” ì˜ì¡´ì„±
fastapi==0.104.1          # ì›¹ í”„ë ˆì„ì›Œí¬
uvicorn==0.24.0           # ASGI ì„œë²„
pydantic==2.4.2           # ë°ì´í„° ê²€ì¦
```

**ì„ ì • ì´ìœ **:
- FastAPI: ë¹ ë¥¸ ê°œë°œ, ìë™ API ë¬¸ì„œí™”, íƒ€ì… íŒíŠ¸ ì§€ì›
- ë¹„ë™ê¸° ì²˜ë¦¬ë¡œ ë†’ì€ ì„±ëŠ¥
- Gradioì™€ ì›í™œí•œ í†µí•©

#### **AI/ML ë¼ì´ë¸ŒëŸ¬ë¦¬**
```python
# LLM ë° ìì—°ì–´ ì²˜ë¦¬
langchain==0.0.350        # LLM ì²´ì¸ ë° ë„êµ¬
openai==1.3.8             # OpenAI API í´ë¼ì´ì–¸íŠ¸
langchain-experimental==0.0.45  # ì‹¤í—˜ì  ê¸°ëŠ¥

# ë°ì´í„° ë¶„ì„
pandas==2.1.4             # ë°ì´í„° ì²˜ë¦¬
numpy==1.24.4             # ìˆ˜ì¹˜ ê³„ì‚°
scipy==1.11.4             # í†µê³„ ë¶„ì„
```

**ì„ ì • ì´ìœ **:
- LangChain: SQL ì²´ì¸, ë„êµ¬ í†µí•©, í”„ë¡¬í”„íŠ¸ ê´€ë¦¬
- ê²€ì¦ëœ ë°ì´í„° ë¶„ì„ ë¼ì´ë¸ŒëŸ¬ë¦¬

#### **ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°**
```python
# ë°ì´í„°ë² ì´ìŠ¤ ë“œë¼ì´ë²„
sqlalchemy==2.0.23        # ORM ë° ì»¤ë„¥ì…˜ í’€
psycopg2-binary==2.9.9    # PostgreSQL ë“œë¼ì´ë²„
pymysql==1.1.0            # MySQL ë“œë¼ì´ë²„
redis==5.0.1              # ìºì‹œ í´ë¼ì´ì–¸íŠ¸
```

#### **ì‹œê°í™” ë° UI**
```python
# ì‹œê°í™”
plotly==5.17.0            # ì¸í„°ë™í‹°ë¸Œ ì°¨íŠ¸
matplotlib==3.8.2         # ê¸°ë³¸ ì°¨íŠ¸ (ë°±ì—…)

# ì‚¬ìš©ì ì¸í„°í˜ì´ìŠ¤
gradio==4.7.1             # ì›¹ UI í”„ë ˆì„ì›Œí¬
```

#### **íŒŒì¼ ì²˜ë¦¬**
```python
# Excel ë° íŒŒì¼ ì²˜ë¦¬
openpyxl==3.1.2           # Excel ì½ê¸°/ì“°ê¸°
xlrd==2.0.1               # êµ¬ë²„ì „ Excel ì§€ì›
chardet==5.2.0            # ì¸ì½”ë”© ê°ì§€
```

#### **ê¸°íƒ€ ìœ í‹¸ë¦¬í‹°**
```python
# í™˜ê²½ ì„¤ì • ë° ë³´ì•ˆ
python-dotenv==1.0.0      # í™˜ê²½ ë³€ìˆ˜ ê´€ë¦¬
cryptography==41.0.7      # ì•”í˜¸í™”
python-jose==3.3.0        # JWT í† í°

# ë¡œê¹… ë° ëª¨ë‹ˆí„°ë§
structlog==23.2.0         # êµ¬ì¡°í™”ëœ ë¡œê¹…
prometheus-client==0.19.0 # ë©”íŠ¸ë¦­ ìˆ˜ì§‘
```

### ì§€ì› ê¸°ìˆ 

#### **ê°œë°œ ë„êµ¬**
```python
# ê°œë°œ ë° í…ŒìŠ¤íŠ¸
pytest==7.4.3            # í…ŒìŠ¤íŠ¸ í”„ë ˆì„ì›Œí¬
black==23.11.0           # ì½”ë“œ í¬ë§¤í„°
flake8==6.1.0            # ë¦°í„°
mypy==1.7.1              # íƒ€ì… ì²´ì»¤
```

#### **ë°°í¬ ë° ì¸í”„ë¼**
```dockerfile
# Docker ê¸°ë°˜ ë°°í¬
FROM python:3.11-slim
# ì»¨í…Œì´ë„ˆ ì„¤ì •
```

## ğŸ›ï¸ ìƒì„¸ ì•„í‚¤í…ì²˜ ì„¤ê³„

### 1. í”„ë ˆì  í…Œì´ì…˜ ê³„ì¸µ (Presentation Layer)

#### 1.1 Gradio ì›¹ ì¸í„°í˜ì´ìŠ¤
```python
# gradio_app.py
import gradio as gr
from typing import Tuple, Optional

class DataGenieUI:
    def __init__(self, backend_service):
        self.backend = backend_service
        self.setup_interface()
    
    def setup_interface(self):
        with gr.Blocks(title="DataGenie", theme=gr.themes.Soft()) as self.app:
            # í—¤ë”
            gr.Markdown("# ğŸ§â€â™‚ï¸ DataGenie - AI ë°ì´í„° ë¶„ì„ ë¹„ì„œ")
            
            with gr.Row():
                with gr.Column(scale=2):
                    # ì§ˆë¬¸ ì…ë ¥
                    question_input = gr.Textbox(
                        label="ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”",
                        placeholder="ì˜ˆ: ì›”ë³„ ë§¤ì¶œ í˜„í™©ì„ ë³´ì—¬ì¤˜",
                        lines=2
                    )
                    
                    # íŒŒì¼ ì—…ë¡œë“œ
                    file_upload = gr.File(
                        label="Excel íŒŒì¼ ì—…ë¡œë“œ",
                        file_types=[".xlsx", ".xls", ".csv"]
                    )
                    
                    submit_btn = gr.Button("ë¶„ì„ ì‹œì‘", variant="primary")
                
                with gr.Column(scale=1):
                    # ì§ˆë¬¸ ì´ë ¥
                    history = gr.Textbox(
                        label="ìµœê·¼ ì§ˆë¬¸",
                        interactive=False,
                        max_lines=10
                    )
            
            # ê²°ê³¼ í‘œì‹œ ì˜ì—­
            with gr.Row():
                with gr.Column():
                    # í…ìŠ¤íŠ¸ ì‘ë‹µ
                    text_output = gr.Markdown(label="ë¶„ì„ ê²°ê³¼")
                    
                    # ì‹œê°í™”
                    plot_output = gr.Plot(label="ì°¨íŠ¸")
                    
                    # ë°ì´í„° í…Œì´ë¸”
                    data_output = gr.Dataframe(label="ìƒì„¸ ë°ì´í„°")
            
            # ì´ë²¤íŠ¸ ë°”ì¸ë”©
            submit_btn.click(
                fn=self.process_question,
                inputs=[question_input, file_upload],
                outputs=[text_output, plot_output, data_output, history]
            )
```

#### 1.2 ë°˜ì‘í˜• ë””ìì¸
```css
/* ì‚¬ìš©ì ì •ì˜ CSS */
.gradio-container {
    max-width: 1200px;
    margin: 0 auto;
}

.question-input {
    font-size: 16px;
    border-radius: 8px;
}

.result-container {
    background: #f8f9fa;
    padding: 20px;
    border-radius: 12px;
    margin-top: 20px;
}

@media (max-width: 768px) {
    .gradio-row {
        flex-direction: column;
    }
}
```

### 2. ì• í”Œë¦¬ì¼€ì´ì…˜ ê³„ì¸µ (Application Layer)

#### 2.1 FastAPI ë°±ì—”ë“œ ì„œë¹„ìŠ¤
```python
# main.py
from fastapi import FastAPI, HTTPException
from contextlib import asynccontextmanager
import uvicorn

@asynccontextmanager
async def lifespan(app: FastAPI):
    # ì‹œì‘ì‹œ ì´ˆê¸°í™”
    await initialize_services()
    yield
    # ì¢…ë£Œì‹œ ì •ë¦¬
    await cleanup_services()

app = FastAPI(
    title="DataGenie API",
    description="LLM ê¸°ë°˜ ë°ì´í„° ë¶„ì„ ì„œë¹„ìŠ¤",
    version="1.0.0",
    lifespan=lifespan
)

# ë¯¸ë“¤ì›¨ì–´ ì„¤ì •
from fastapi.middleware.cors import CORSMiddleware
from fastapi.middleware.gzip import GZipMiddleware

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_methods=["*"],
    allow_headers=["*"],
)
app.add_middleware(GZipMiddleware, minimum_size=1000)

# ë¼ìš°í„° ë“±ë¡
from routes import analysis, health, auth
app.include_router(analysis.router, prefix="/api/analysis")
app.include_router(health.router, prefix="/api/health")
app.include_router(auth.router, prefix="/api/auth")
```

#### 2.2 ìš”ì²­ ì²˜ë¦¬ í”Œë¡œìš°
```python
# services/analysis_service.py
from typing import Dict, Any, Optional
import asyncio

class AnalysisService:
    def __init__(self):
        self.nlp_processor = NLPProcessor()
        self.query_engine = QueryEngine()
        self.excel_engine = ExcelEngine()
        self.viz_engine = VisualizationEngine()
    
    async def process_question(
        self, 
        question: str, 
        file_data: Optional[bytes] = None,
        user_context: Dict[str, Any] = None
    ) -> Dict[str, Any]:
        """ë©”ì¸ ì§ˆë¬¸ ì²˜ë¦¬ ì›Œí¬í”Œë¡œìš°"""
        
        try:
            # 1. ì§ˆë¬¸ ë¶„ì„ ë° ë¼ìš°íŒ…
            analysis_result = await self.nlp_processor.analyze_question(question)
            
            # 2. ì²˜ë¦¬ ìœ í˜•ì— ë”°ë¥¸ ë¶„ê¸°
            if analysis_result.type == "DB_QUERY":
                result = await self._process_db_query(question, analysis_result)
            elif analysis_result.type == "EXCEL_ANALYSIS":
                result = await self._process_excel_analysis(question, file_data)
            else:
                result = await self._process_general_question(question)
            
            # 3. ì‘ë‹µ ìƒì„±
            response = await self._generate_response(result, analysis_result)
            
            return response
            
        except Exception as e:
            logger.error(f"ë¶„ì„ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
            return self._create_error_response(str(e))
    
    async def _process_db_query(self, question: str, analysis: Any) -> Dict:
        """ë°ì´í„°ë² ì´ìŠ¤ ì¿¼ë¦¬ ì²˜ë¦¬"""
        # SQL ìƒì„±
        sql_query = await self.query_engine.generate_sql(question, analysis)
        
        # ì¿¼ë¦¬ ì‹¤í–‰
        data = await self.query_engine.execute_query(sql_query)
        
        # ì‹œê°í™” ìƒì„±
        visualization = await self.viz_engine.create_visualization(data)
        
        return {
            "type": "database",
            "sql_query": sql_query,
            "data": data,
            "visualization": visualization
        }
```

### 3. ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§ ê³„ì¸µ (Business Logic Layer)

#### 3.1 ìì—°ì–´ ì²˜ë¦¬ ëª¨ë“ˆ
```python
# nlp/processor.py
from langchain.llms import OpenAI
from langchain.prompts import PromptTemplate
from pydantic import BaseModel
from typing import Literal

class QuestionAnalysis(BaseModel):
    type: Literal["DB_QUERY", "EXCEL_ANALYSIS", "GENERAL"]
    intent: str
    entities: Dict[str, Any]
    confidence: float
    suggested_tables: List[str] = []

class NLPProcessor:
    def __init__(self):
        self.llm = OpenAI(temperature=0)
        self.classification_prompt = PromptTemplate(
            input_variables=["question"],
            template="""
ë‹¤ìŒ ì‚¬ìš©ì ì§ˆë¬¸ì„ ë¶„ì„í•˜ì—¬ ì²˜ë¦¬ ìœ í˜•ì„ ë¶„ë¥˜í•˜ì„¸ìš”.

ì§ˆë¬¸: {question}

ë¶„ë¥˜ ê¸°ì¤€:
- DB_QUERY: ë°ì´í„°ë² ì´ìŠ¤ ì¡°íšŒê°€ í•„ìš”í•œ ì§ˆë¬¸
- EXCEL_ANALYSIS: ì—…ë¡œë“œëœ ì—‘ì…€ íŒŒì¼ ë¶„ì„ ì§ˆë¬¸
- GENERAL: ì¼ë°˜ì ì¸ ì§ˆë¬¸ì´ë‚˜ ë„ì›€ë§ ìš”ì²­

ì‘ë‹µ í˜•ì‹:
{{
  "type": "DB_QUERY|EXCEL_ANALYSIS|GENERAL",
  "intent": "ì‚¬ìš©ì ì˜ë„ ì„¤ëª…",
  "entities": {{"ì¶”ì¶œëœ ì—”í‹°í‹°": "ê°’"}},
  "confidence": 0.95
}}
"""
        )
    
    async def analyze_question(self, question: str) -> QuestionAnalysis:
        """ì§ˆë¬¸ ë¶„ì„ ë° ë¶„ë¥˜"""
        try:
            # LLMì„ í†µí•œ ì§ˆë¬¸ ë¶„ì„
            prompt = self.classification_prompt.format(question=question)
            response = await self.llm.agenerate([prompt])
            
            # ì‘ë‹µ íŒŒì‹±
            result = self._parse_llm_response(response.generations[0][0].text)
            
            # ì¶”ê°€ ë¶„ì„ (í…Œì´ë¸” ì¶”ì²œ ë“±)
            if result.type == "DB_QUERY":
                result.suggested_tables = await self._suggest_tables(question)
            
            return result
            
        except Exception as e:
            logger.error(f"ì§ˆë¬¸ ë¶„ì„ ì˜¤ë¥˜: {e}")
            return QuestionAnalysis(
                type="GENERAL",
                intent="ë¶„ì„ ì‹¤íŒ¨",
                entities={},
                confidence=0.0
            )
```

#### 3.2 ì¿¼ë¦¬ ì—”ì§„
```python
# query/engine.py
from langchain.sql_database import SQLDatabase
from langchain.agents import create_sql_agent
from langchain.agents.agent_toolkits import SQLDatabaseToolkit
from sqlalchemy import create_engine, text
from typing import Dict, List, Any

class QueryEngine:
    def __init__(self):
        self.databases = {}
        self.agents = {}
        self._initialize_connections()
    
    def _initialize_connections(self):
        """ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì´ˆê¸°í™”"""
        for db_name, config in DATABASE_CONFIGS.items():
            try:
                # SQLAlchemy ì—”ì§„ ìƒì„±
                engine = create_engine(
                    f"{config['type']}://{config['username']}:{config['password']}"
                    f"@{config['host']}:{config['port']}/{config['database']}",
                    pool_size=10,
                    max_overflow=20,
                    pool_pre_ping=True
                )
                
                # LangChain SQLDatabase ë˜í¼
                db = SQLDatabase(engine)
                self.databases[db_name] = db
                
                # SQL Agent ìƒì„±
                toolkit = SQLDatabaseToolkit(db=db, llm=self.llm)
                agent = create_sql_agent(
                    llm=self.llm,
                    toolkit=toolkit,
                    verbose=True,
                    agent_type="zero-shot-react-description"
                )
                self.agents[db_name] = agent
                
                logger.info(f"ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì„±ê³µ: {db_name}")
                
            except Exception as e:
                logger.error(f"ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì‹¤íŒ¨ {db_name}: {e}")
    
    async def generate_sql(self, question: str, analysis: QuestionAnalysis) -> str:
        """ìì—°ì–´ ì§ˆë¬¸ì„ SQLë¡œ ë³€í™˜"""
        try:
            # ê¸°ë³¸ ë°ì´í„°ë² ì´ìŠ¤ ì„ íƒ
            db_name = "main_db"  # ì„¤ì •ì—ì„œ ê°€ì ¸ì˜¬ ìˆ˜ë„ ìˆìŒ
            agent = self.agents[db_name]
            
            # SQL ìƒì„± í”„ë¡¬í”„íŠ¸ êµ¬ì„±
            enhanced_question = f"""
ë‹¤ìŒ ì§ˆë¬¸ì— ëŒ€í•œ SQL ì¿¼ë¦¬ë¥¼ ìƒì„±í•˜ì„¸ìš”:
{question}

ì œì•½ì‚¬í•­:
- SELECT ì¿¼ë¦¬ë§Œ í—ˆìš© (INSERT, UPDATE, DELETE ê¸ˆì§€)
- ê²°ê³¼ í–‰ ìˆ˜ë¥¼ 1000ê°œë¡œ ì œí•œ
- ê°œì¸ì •ë³´ê°€ í¬í•¨ëœ ì»¬ëŸ¼ì€ ë§ˆìŠ¤í‚¹ ì²˜ë¦¬
- ì‹¤í–‰ ì‹œê°„ì´ 30ì´ˆë¥¼ ì´ˆê³¼í•˜ì§€ ì•Šë„ë¡ ìµœì í™”

ì¶”ê°€ ì»¨í…ìŠ¤íŠ¸:
- ë¶„ì„ ìœ í˜•: {analysis.intent}
- ì¶”ì¶œëœ ì—”í‹°í‹°: {analysis.entities}
"""
            
            # Agentë¥¼ í†µí•œ SQL ìƒì„±
            result = await agent.arun(enhanced_question)
            
            # SQL ì¶”ì¶œ ë° ê²€ì¦
            sql_query = self._extract_sql_from_result(result)
            validated_sql = self._validate_sql(sql_query)
            
            return validated_sql
            
        except Exception as e:
            logger.error(f"SQL ìƒì„± ì˜¤ë¥˜: {e}")
            raise HTTPException(status_code=400, detail=f"SQL ìƒì„± ì‹¤íŒ¨: {e}")
    
    async def execute_query(self, sql_query: str, db_name: str = "main_db") -> Dict[str, Any]:
        """SQL ì¿¼ë¦¬ ì‹¤í–‰"""
        try:
            db = self.databases[db_name]
            
            # ì¿¼ë¦¬ ì‹¤í–‰ (íƒ€ì„ì•„ì›ƒ ì„¤ì •)
            with asyncio.timeout(30):  # 30ì´ˆ íƒ€ì„ì•„ì›ƒ
                result = db.run(sql_query)
            
            # ê²°ê³¼ ì²˜ë¦¬
            processed_result = self._process_query_result(result)
            
            return {
                "sql": sql_query,
                "data": processed_result["data"],
                "columns": processed_result["columns"],
                "row_count": processed_result["row_count"],
                "execution_time": processed_result["execution_time"]
            }
            
        except asyncio.TimeoutError:
            raise HTTPException(status_code=408, detail="ì¿¼ë¦¬ ì‹¤í–‰ ì‹œê°„ ì´ˆê³¼")
        except Exception as e:
            logger.error(f"ì¿¼ë¦¬ ì‹¤í–‰ ì˜¤ë¥˜: {e}")
            raise HTTPException(status_code=500, detail=f"ì¿¼ë¦¬ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
```

#### 3.3 Excel ë¶„ì„ ì—”ì§„
```python
# excel/engine.py
import pandas as pd
import io
from typing import Dict, Any, Optional
import asyncio
from concurrent.futures import ThreadPoolExecutor

class ExcelEngine:
    def __init__(self):
        self.executor = ThreadPoolExecutor(max_workers=4)
        self.active_sessions = {}  # ì„¸ì…˜ë³„ ë°ì´í„° ì €ì¥
    
    async def process_file(self, file_data: bytes, session_id: str) -> Dict[str, Any]:
        """Excel íŒŒì¼ ì²˜ë¦¬ ë° ë¶„ì„"""
        try:
            # ë¹„ë™ê¸° íŒŒì¼ ì²˜ë¦¬
            loop = asyncio.get_event_loop()
            result = await loop.run_in_executor(
                self.executor, 
                self._process_file_sync, 
                file_data
            )
            
            # ì„¸ì…˜ì— ë°ì´í„° ì €ì¥
            self.active_sessions[session_id] = {
                "dataframe": result["dataframe"],
                "metadata": result["metadata"],
                "created_at": asyncio.get_event_loop().time()
            }
            
            return result
            
        except Exception as e:
            logger.error(f"Excel íŒŒì¼ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
            raise HTTPException(status_code=400, detail=f"íŒŒì¼ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
    
    def _process_file_sync(self, file_data: bytes) -> Dict[str, Any]:
        """ë™ê¸° íŒŒì¼ ì²˜ë¦¬ (ë³„ë„ ìŠ¤ë ˆë“œì—ì„œ ì‹¤í–‰)"""
        # íŒŒì¼ í˜•ì‹ ê°ì§€
        file_obj = io.BytesIO(file_data)
        
        try:
            # Excel íŒŒì¼ ì½ê¸°
            if file_data[:4] == b'PK\x03\x04':  # xlsx ì‹œê·¸ë‹ˆì²˜
                df = pd.read_excel(file_obj, engine='openpyxl')
            else:
                # CSV ì‹œë„
                file_obj.seek(0)
                encoding = self._detect_encoding(file_data)
                df = pd.read_csv(file_obj, encoding=encoding)
            
            # ë°ì´í„° ê²€ì¦
            if len(df) > 1_000_000:
                raise ValueError("íŒŒì¼ í¬ê¸°ê°€ ë„ˆë¬´ í½ë‹ˆë‹¤ (ìµœëŒ€ 100ë§Œ í–‰)")
            
            # ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
            metadata = self._extract_metadata(df)
            
            return {
                "dataframe": df,
                "metadata": metadata,
                "preview": df.head(10).to_dict('records'),
                "shape": df.shape,
                "columns": df.columns.tolist(),
                "dtypes": df.dtypes.to_dict()
            }
            
        except Exception as e:
            raise ValueError(f"íŒŒì¼ íŒŒì‹± ì˜¤ë¥˜: {e}")
    
    async def analyze_data(self, question: str, session_id: str) -> Dict[str, Any]:
        """ë°ì´í„° ë¶„ì„ ìˆ˜í–‰"""
        if session_id not in self.active_sessions:
            raise HTTPException(status_code=404, detail="ì„¸ì…˜ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        
        df = self.active_sessions[session_id]["dataframe"]
        metadata = self.active_sessions[session_id]["metadata"]
        
        try:
            # Pandas ì½”ë“œ ìƒì„±
            analysis_code = await self._generate_analysis_code(question, df, metadata)
            
            # ì½”ë“œ ì‹¤í–‰
            result = await self._execute_analysis_code(analysis_code, df)
            
            return {
                "question": question,
                "code": analysis_code,
                "result": result,
                "data_summary": self._summarize_result(result)
            }
            
        except Exception as e:
            logger.error(f"ë°ì´í„° ë¶„ì„ ì˜¤ë¥˜: {e}")
            raise HTTPException(status_code=500, detail=f"ë¶„ì„ ì‹¤íŒ¨: {e}")
```

#### 3.4 ì‹œê°í™” ì—”ì§„
```python
# visualization/engine.py
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import pandas as pd
from typing import Dict, Any, Optional, Literal

class VisualizationEngine:
    def __init__(self):
        self.chart_configs = self._load_chart_configurations()
    
    async def create_visualization(
        self, 
        data: pd.DataFrame, 
        chart_type: Optional[str] = None,
        question_context: Optional[str] = None
    ) -> Dict[str, Any]:
        """ë°ì´í„° ì‹œê°í™” ìƒì„±"""
        
        try:
            # ì°¨íŠ¸ ìœ í˜• ìë™ ê²°ì •
            if not chart_type:
                chart_type = self._recommend_chart_type(data, question_context)
            
            # ì°¨íŠ¸ ìƒì„±
            fig = await self._create_chart(data, chart_type)
            
            # í•œê¸€ í°íŠ¸ ë° í…Œë§ˆ ì ìš©
            fig.update_layout(
                font_family="Malgun Gothic, Arial",
                template="plotly_white",
                title_font_size=16,
                showlegend=True
            )
            
            return {
                "chart_type": chart_type,
                "figure": fig.to_json(),
                "html": fig.to_html(include_plotlyjs='cdn'),
                "config": {
                    "displayModeBar": True,
                    "displaylogo": False,
                    "modeBarButtonsToRemove": ['pan2d', 'lasso2d']
                }
            }
            
        except Exception as e:
            logger.error(f"ì‹œê°í™” ìƒì„± ì˜¤ë¥˜: {e}")
            return self._create_fallback_table(data)
    
    def _recommend_chart_type(self, data: pd.DataFrame, context: str) -> str:
        """ë°ì´í„° íŠ¹ì„±ì— ë”°ë¥¸ ì°¨íŠ¸ ìœ í˜• ì¶”ì²œ"""
        
        # ë°ì´í„° ë¶„ì„
        numeric_columns = data.select_dtypes(include=['number']).columns
        datetime_columns = data.select_dtypes(include=['datetime']).columns
        categorical_columns = data.select_dtypes(include=['object']).columns
        
        # ì‹œê³„ì—´ ë°ì´í„° ê°ì§€
        if len(datetime_columns) > 0 and len(numeric_columns) > 0:
            return "line"
        
        # ì¹´í…Œê³ ë¦¬ë³„ ì§‘ê³„ ë°ì´í„°
        if len(categorical_columns) > 0 and len(numeric_columns) > 0:
            if data.shape[0] <= 20:  # ì¹´í…Œê³ ë¦¬ê°€ ì ìœ¼ë©´
                return "bar"
            else:
                return "histogram"
        
        # ë‘ ìˆ˜ì¹˜í˜• ë³€ìˆ˜
        if len(numeric_columns) >= 2:
            return "scatter"
        
        # ë‹¨ì¼ ìˆ˜ì¹˜í˜• ë³€ìˆ˜ ë¶„í¬
        if len(numeric_columns) == 1:
            return "histogram"
        
        # ê¸°ë³¸ê°’
        return "table"
    
    async def _create_chart(self, data: pd.DataFrame, chart_type: str) -> go.Figure:
        """ì°¨íŠ¸ ìœ í˜•ë³„ ìƒì„±"""
        
        if chart_type == "bar":
            return self._create_bar_chart(data)
        elif chart_type == "line":
            return self._create_line_chart(data)
        elif chart_type == "pie":
            return self._create_pie_chart(data)
        elif chart_type == "scatter":
            return self._create_scatter_chart(data)
        elif chart_type == "histogram":
            return self._create_histogram(data)
        else:
            return self._create_table_chart(data)
    
    def _create_bar_chart(self, data: pd.DataFrame) -> go.Figure:
        """ë§‰ëŒ€ ê·¸ë˜í”„ ìƒì„±"""
        # ì²« ë²ˆì§¸ í…ìŠ¤íŠ¸ ì»¬ëŸ¼ì„ xì¶•, ì²« ë²ˆì§¸ ìˆ«ì ì»¬ëŸ¼ì„ yì¶•ìœ¼ë¡œ
        text_col = data.select_dtypes(include=['object']).columns[0]
        numeric_col = data.select_dtypes(include=['number']).columns[0]
        
        fig = px.bar(
            data, 
            x=text_col, 
            y=numeric_col,
            title=f"{text_col}ë³„ {numeric_col}",
            color=numeric_col,
            color_continuous_scale="Blues"
        )
        
        fig.update_xaxes(tickangle=45)
        return fig
```

### 4. ë°ì´í„° ê³„ì¸µ (Data Layer)

#### 4.1 ìºì‹œ ì‹œìŠ¤í…œ (Redis)
```python
# cache/redis_client.py
import redis.asyncio as redis
import json
import pickle
from typing import Any, Optional, Union
import hashlib

class CacheManager:
    def __init__(self):
        self.redis_client = redis.from_url(
            "redis://localhost:6379/0",
            encoding="utf-8",
            decode_responses=False
        )
    
    async def get_query_result(self, sql_query: str) -> Optional[Dict[str, Any]]:
        """ì¿¼ë¦¬ ê²°ê³¼ ìºì‹œ ì¡°íšŒ"""
        cache_key = self._generate_cache_key("sql", sql_query)
        
        try:
            cached_data = await self.redis_client.get(cache_key)
            if cached_data:
                return pickle.loads(cached_data)
            return None
        except Exception as e:
            logger.warning(f"ìºì‹œ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return None
    
    async def set_query_result(
        self, 
        sql_query: str, 
        result: Dict[str, Any], 
        ttl: int = 3600
    ):
        """ì¿¼ë¦¬ ê²°ê³¼ ìºì‹œ ì €ì¥"""
        cache_key = self._generate_cache_key("sql", sql_query)
        
        try:
            serialized_data = pickle.dumps(result)
            await self.redis_client.setex(cache_key, ttl, serialized_data)
        except Exception as e:
            logger.warning(f"ìºì‹œ ì €ì¥ ì‹¤íŒ¨: {e}")
    
    def _generate_cache_key(self, prefix: str, content: str) -> str:
        """ìºì‹œ í‚¤ ìƒì„±"""
        content_hash = hashlib.md5(content.encode()).hexdigest()
        return f"datagenie:{prefix}:{content_hash}"
```

#### 4.2 ì„¤ì • ê´€ë¦¬
```python
# config/settings.py
from pydantic import BaseSettings
from typing import Dict, Any

class Settings(BaseSettings):
    # ì• í”Œë¦¬ì¼€ì´ì…˜ ì„¤ì •
    app_name: str = "DataGenie"
    app_version: str = "1.0.0"
    debug: bool = False
    
    # OpenAI ì„¤ì •
    openai_api_key: str
    openai_model: str = "gpt-4"
    openai_temperature: float = 0.0
    
    # ë°ì´í„°ë² ì´ìŠ¤ ì„¤ì •
    database_configs: Dict[str, Any] = {
        "main_db": {
            "type": "postgresql",
            "host": "localhost",
            "port": 5432,
            "database": "company_db",
            "username": "readonly_user",
            "password": "",
        }
    }
    
    # Redis ì„¤ì •
    redis_url: str = "redis://localhost:6379/0"
    
    # íŒŒì¼ ì„¤ì •
    max_file_size: int = 50 * 1024 * 1024  # 50MB
    max_rows: int = 1_000_000
    upload_dir: str = "/tmp/datagenie"
    
    # ë³´ì•ˆ ì„¤ì •
    secret_key: str
    jwt_algorithm: str = "HS256"
    jwt_expire_hours: int = 24
    
    class Config:
        env_file = ".env"
        env_file_encoding = "utf-8"

settings = Settings()
```

## ğŸ”„ ë°ì´í„° í”Œë¡œìš°

### 1. ì¼ë°˜ì ì¸ ì§ˆì˜ í”Œë¡œìš°
```
ì‚¬ìš©ì ì§ˆë¬¸ ì…ë ¥
       â†“
Gradio UI â†’ FastAPI
       â†“
ì§ˆë¬¸ ë¶„ì„ (NLP)
       â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â†“         â†“         â†“
 DB ì¿¼ë¦¬   Excel ë¶„ì„  ì¼ë°˜ì‘ë‹µ
    â†“         â†“         â†“
ìºì‹œ í™•ì¸   íŒŒì¼ ì²˜ë¦¬   LLM ì‘ë‹µ
    â†“         â†“         â†“
SQL ì‹¤í–‰   ì½”ë“œ ì‹¤í–‰   í˜•ì‹í™”
    â†“         â†“         â†“
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â†“
       ì‹œê°í™” ìƒì„±
             â†“
       ì‘ë‹µ ì¡°í•©
             â†“
     Gradio UI ì¶œë ¥
```

### 2. ìºì‹± ì „ëµ
```python
# ìºì‹œ ê³„ì¸µ êµ¬ì¡°
Level 1: ë©”ëª¨ë¦¬ ìºì‹œ (ë¹ ë¥¸ ì ‘ê·¼, ì œí•œëœ ìš©ëŸ‰)
Level 2: Redis ìºì‹œ (ì¤‘ê°„ ì†ë„, ì„¸ì…˜ ê³µìœ )
Level 3: ë°ì´í„°ë² ì´ìŠ¤ (ëŠë¦° ì†ë„, ì˜êµ¬ ì €ì¥)

ìºì‹œ í‚¤ êµ¬ì¡°:
- ì¿¼ë¦¬ ê²°ê³¼: "datagenie:sql:{hash}"
- íŒŒì¼ ë¶„ì„: "datagenie:excel:{session_id}:{hash}"
- LLM ì‘ë‹µ: "datagenie:llm:{prompt_hash}"
```

## ğŸ›¡ï¸ ë³´ì•ˆ ì•„í‚¤í…ì²˜

### 1. ì¸ì¦ ë° ê¶Œí•œ
```python
# auth/security.py
from fastapi import Depends, HTTPException, status
from fastapi.security import HTTPBearer, HTTPAuthorizationCredentials
from jose import jwt, JWTError
import bcrypt

class SecurityManager:
    def __init__(self):
        self.security = HTTPBearer()
    
    async def get_current_user(
        self, 
        credentials: HTTPAuthorizationCredentials = Depends(HTTPBearer())
    ) -> Dict[str, Any]:
        """JWT í† í° ê²€ì¦ ë° ì‚¬ìš©ì ì •ë³´ ë°˜í™˜"""
        try:
            payload = jwt.decode(
                credentials.credentials, 
                settings.secret_key, 
                algorithms=[settings.jwt_algorithm]
            )
            user_id = payload.get("sub")
            if user_id is None:
                raise HTTPException(
                    status_code=status.HTTP_401_UNAUTHORIZED,
                    detail="Invalid authentication credentials"
                )
            return {"user_id": user_id, "permissions": payload.get("permissions", [])}
        except JWTError:
            raise HTTPException(
                status_code=status.HTTP_401_UNAUTHORIZED,
                detail="Invalid authentication credentials"
            )
```

### 2. SQL Injection ë°©ì§€
```python
# security/sql_validator.py
import sqlparse
from typing import List

class SQLValidator:
    FORBIDDEN_KEYWORDS = [
        'DROP', 'DELETE', 'INSERT', 'UPDATE', 'ALTER', 'CREATE',
        'TRUNCATE', 'EXEC', 'EXECUTE', 'MERGE', 'CALL'
    ]
    
    def validate_sql(self, sql: str) -> bool:
        """SQL ì¿¼ë¦¬ ì•ˆì „ì„± ê²€ì¦"""
        try:
            # íŒŒì‹± ì‹œë„
            parsed = sqlparse.parse(sql)
            
            for statement in parsed:
                # DML/DDL ëª…ë ¹ì–´ ê²€ì‚¬
                if self._contains_forbidden_keywords(statement):
                    raise ValueError("ìœ„í—˜í•œ SQL ëª…ë ¹ì–´ ê°ì§€")
                
                # ì„œë¸Œì¿¼ë¦¬ ê¹Šì´ ì œí•œ
                if self._count_subquery_depth(statement) > 3:
                    raise ValueError("ì„œë¸Œì¿¼ë¦¬ ê¹Šì´ ì´ˆê³¼")
            
            return True
            
        except Exception as e:
            logger.warning(f"SQL ê²€ì¦ ì‹¤íŒ¨: {e}")
            return False
```

## ğŸ“Š ëª¨ë‹ˆí„°ë§ ë° ë¡œê¹…

### 1. ë¡œê¹… êµ¬ì¡°
```python
# logging/config.py
import structlog
import logging

def setup_logging():
    logging.basicConfig(
        format="%(message)s",
        stream=sys.stdout,
        level=logging.INFO,
    )
    
    structlog.configure(
        processors=[
            structlog.stdlib.filter_by_level,
            structlog.stdlib.add_log_level,
            structlog.stdlib.add_logger_name,
            structlog.stdlib.PositionalArgumentsFormatter(),
            structlog.processors.TimeStamper(fmt="iso"),
            structlog.processors.StackInfoRenderer(),
            structlog.processors.format_exc_info,
            structlog.processors.UnicodeDecoder(),
            structlog.processors.JSONRenderer()
        ],
        context_class=dict,
        logger_factory=structlog.stdlib.LoggerFactory(),
        wrapper_class=structlog.stdlib.BoundLogger,
        cache_logger_on_first_use=True,
    )

logger = structlog.get_logger()
```

### 2. ë©”íŠ¸ë¦­ ìˆ˜ì§‘
```python
# monitoring/metrics.py
from prometheus_client import Counter, Histogram, Gauge

# ìš”ì²­ ë©”íŠ¸ë¦­
request_count = Counter(
    'datagenie_requests_total',
    'Total requests',
    ['method', 'endpoint', 'status']
)

request_duration = Histogram(
    'datagenie_request_duration_seconds',
    'Request duration',
    ['method', 'endpoint']
)

# ë¹„ì¦ˆë‹ˆìŠ¤ ë©”íŠ¸ë¦­
query_count = Counter(
    'datagenie_queries_total',
    'Total queries processed',
    ['type', 'status']
)

active_sessions = Gauge(
    'datagenie_active_sessions',
    'Number of active sessions'
)

# LLM ë©”íŠ¸ë¦­
llm_tokens = Counter(
    'datagenie_llm_tokens_total',
    'Total LLM tokens used',
    ['model', 'type']
)
```

## ğŸš€ ë°°í¬ ì•„í‚¤í…ì²˜

### 1. Docker ì»¨í…Œì´ë„ˆ êµ¬ì„±
```dockerfile
# Dockerfile
FROM python:3.11-slim

WORKDIR /app

# ì‹œìŠ¤í…œ ì˜ì¡´ì„± ì„¤ì¹˜
RUN apt-get update && apt-get install -y \
    gcc \
    g++ \
    && rm -rf /var/lib/apt/lists/*

# Python ì˜ì¡´ì„± ì„¤ì¹˜
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# ì• í”Œë¦¬ì¼€ì´ì…˜ ì½”ë“œ
COPY . .

# í¬íŠ¸ ë…¸ì¶œ
EXPOSE 8000

# í—¬ìŠ¤ì²´í¬
HEALTHCHECK --interval=30s --timeout=30s --start-period=5s --retries=3 \
    CMD curl -f http://localhost:8000/api/health || exit 1

# ì‹¤í–‰ ëª…ë ¹
CMD ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8000"]
```

### 2. Docker Compose êµ¬ì„±
```yaml
# docker-compose.yml
version: '3.8'

services:
  app:
    build: .
    ports:
      - "8000:8000"
    environment:
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - DATABASE_URL=${DATABASE_URL}
      - REDIS_URL=redis://redis:6379/0
    depends_on:
      - redis
      - postgres
    volumes:
      - ./uploads:/app/uploads
    restart: unless-stopped

  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
    restart: unless-stopped

  postgres:
    image: postgres:15
    environment:
      - POSTGRES_DB=datagenie
      - POSTGRES_USER=postgres
      - POSTGRES_PASSWORD=password
    ports:
      - "5432:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data
    restart: unless-stopped

volumes:
  redis_data:
  postgres_data:
```

## ğŸ”§ ê°œë°œ í™˜ê²½ ì„¤ì •

### 1. í”„ë¡œì íŠ¸ êµ¬ì¡°
```
datagenie/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ main.py                 # FastAPI ì•± ì—”íŠ¸ë¦¬í¬ì¸íŠ¸
â”‚   â”œâ”€â”€ config/
â”‚   â”‚   â”œâ”€â”€ settings.py         # ì„¤ì • ê´€ë¦¬
â”‚   â”‚   â””â”€â”€ database.py         # DB ì—°ê²° ì„¤ì •
â”‚   â”œâ”€â”€ nlp/
â”‚   â”‚   â”œâ”€â”€ processor.py        # ìì—°ì–´ ì²˜ë¦¬
â”‚   â”‚   â””â”€â”€ prompts.py          # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
â”‚   â”œâ”€â”€ query/
â”‚   â”‚   â”œâ”€â”€ engine.py           # ì¿¼ë¦¬ ì—”ì§„
â”‚   â”‚   â””â”€â”€ validator.py        # SQL ê²€ì¦
â”‚   â”œâ”€â”€ excel/
â”‚   â”‚   â”œâ”€â”€ engine.py           # Excel ì²˜ë¦¬
â”‚   â”‚   â””â”€â”€ analyzer.py         # ë°ì´í„° ë¶„ì„
â”‚   â”œâ”€â”€ visualization/
â”‚   â”‚   â”œâ”€â”€ engine.py           # ì‹œê°í™” ìƒì„±
â”‚   â”‚   â””â”€â”€ templates.py        # ì°¨íŠ¸ í…œí”Œë¦¿
â”‚   â”œâ”€â”€ auth/
â”‚   â”‚   â””â”€â”€ security.py         # ì¸ì¦/ë³´ì•ˆ
â”‚   â”œâ”€â”€ cache/
â”‚   â”‚   â””â”€â”€ redis_client.py     # ìºì‹œ ê´€ë¦¬
â”‚   â””â”€â”€ ui/
â”‚       â””â”€â”€ gradio_app.py       # Gradio UI
â”œâ”€â”€ tests/                      # í…ŒìŠ¤íŠ¸ ì½”ë“œ
â”œâ”€â”€ docs/                       # ë¬¸ì„œ
â”œâ”€â”€ requirements.txt            # Python ì˜ì¡´ì„±
â”œâ”€â”€ docker-compose.yml          # ê°œë°œ í™˜ê²½
â”œâ”€â”€ Dockerfile                  # ì»¨í…Œì´ë„ˆ ì´ë¯¸ì§€
â””â”€â”€ README.md                   # í”„ë¡œì íŠ¸ ê°€ì´ë“œ
```

### 2. ê°œë°œ ìŠ¤í¬ë¦½íŠ¸
```bash
#!/bin/bash
# scripts/dev-setup.sh

echo "DataGenie ê°œë°œ í™˜ê²½ ì„¤ì • ì‹œì‘..."

# Python ê°€ìƒí™˜ê²½ ìƒì„±
python -m venv venv
source venv/bin/activate

# ì˜ì¡´ì„± ì„¤ì¹˜
pip install -r requirements.txt
pip install -r requirements-dev.txt

# í™˜ê²½ë³€ìˆ˜ íŒŒì¼ ìƒì„±
if [ ! -f .env ]; then
    cp .env.example .env
    echo "âœ… .env íŒŒì¼ì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤. API í‚¤ë¥¼ ì„¤ì •í•˜ì„¸ìš”."
fi

# Docker ì„œë¹„ìŠ¤ ì‹œì‘
docker-compose up -d redis postgres

# ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”
python scripts/init_db.py

echo "ğŸ‰ ê°œë°œ í™˜ê²½ ì„¤ì • ì™„ë£Œ!"
echo "ë‹¤ìŒ ëª…ë ¹ìœ¼ë¡œ ì„œë²„ë¥¼ ì‹œì‘í•˜ì„¸ìš”:"
echo "uvicorn app.main:app --reload --host 0.0.0.0 --port 8000"
```

---

**ë¬¸ì„œ ìŠ¹ì¸**: âœ… ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜ ì„¤ê³„ ì™„ë£Œ  
**ë‹¤ìŒ ë‹¨ê³„**: ë°ì´í„°ë² ì´ìŠ¤ ì„¤ê³„ì„œ ì‘ì„±
